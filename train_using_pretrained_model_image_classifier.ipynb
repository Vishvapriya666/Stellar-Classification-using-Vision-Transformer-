{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb1c10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from helper_functions import set_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e97c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de25b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "# 4. Change the classifier head \n",
    "class_names = ['galaxy','qso', 'stars']\n",
    "\n",
    "set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3feaa42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 3]              768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
       "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
       "├─Linear (heads)                                             [32, 768]            [32, 3]              2,307                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,800,963\n",
       "Trainable params: 2,307\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.GIGABYTES): 5.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.74\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 3579.21\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ec300",
   "metadata": {},
   "source": [
    "#### Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8cc699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "\n",
    "train_dir = './stellar_images/train'\n",
    "test_dir = './stellar_images/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91175306",
   "metadata": {},
   "source": [
    "Here, we are going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05aa777b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088971e6",
   "metadata": {},
   "source": [
    "## And now we've got transforms ready, we can turn our images into DataLoaders using the create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d49225b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9037c8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c5ba74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39bbac81f604ecf9844b9e0f5f59c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.7939 | train_acc: 0.6255 | test_loss: 0.5411 | test_acc: 0.7981\n",
      "Epoch: 2 | train_loss: 0.4810 | train_acc: 0.8032 | test_loss: 0.4305 | test_acc: 0.8025\n",
      "Epoch: 3 | train_loss: 0.4121 | train_acc: 0.8394 | test_loss: 0.3893 | test_acc: 0.8494\n",
      "Epoch: 4 | train_loss: 0.3631 | train_acc: 0.8712 | test_loss: 0.3638 | test_acc: 0.8572\n",
      "Epoch: 5 | train_loss: 0.3364 | train_acc: 0.8821 | test_loss: 0.3510 | test_acc: 0.8806\n",
      "Epoch: 6 | train_loss: 0.3120 | train_acc: 0.8964 | test_loss: 0.3343 | test_acc: 0.8941\n",
      "Epoch: 7 | train_loss: 0.2886 | train_acc: 0.9019 | test_loss: 0.3324 | test_acc: 0.8572\n",
      "Epoch: 8 | train_loss: 0.2747 | train_acc: 0.9101 | test_loss: 0.3184 | test_acc: 0.9041\n",
      "Epoch: 9 | train_loss: 0.2609 | train_acc: 0.9106 | test_loss: 0.3108 | test_acc: 0.8884\n",
      "Epoch: 10 | train_loss: 0.2448 | train_acc: 0.9128 | test_loss: 0.3149 | test_acc: 0.8884\n",
      "Epoch: 11 | train_loss: 0.2306 | train_acc: 0.9183 | test_loss: 0.3025 | test_acc: 0.8884\n",
      "Epoch: 12 | train_loss: 0.2199 | train_acc: 0.9348 | test_loss: 0.3016 | test_acc: 0.8806\n",
      "Epoch: 13 | train_loss: 0.2122 | train_acc: 0.9293 | test_loss: 0.2870 | test_acc: 0.8962\n",
      "Epoch: 14 | train_loss: 0.2027 | train_acc: 0.9380 | test_loss: 0.3016 | test_acc: 0.8806\n",
      "Epoch: 15 | train_loss: 0.1983 | train_acc: 0.9315 | test_loss: 0.2871 | test_acc: 0.8941\n"
     ]
    }
   ],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "set_seeds()\n",
    "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=15,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945149b4",
   "metadata": {},
   "source": [
    "pretrained ViT performed far better than our custom ViT model trained from scratch (in the same amount of time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae16a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "#from helper_functions import plot_loss_curves\n",
    "\n",
    "#plot_loss_curves(pretrained_vit_results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79922fd1",
   "metadata": {},
   "source": [
    "## We performed of transfer learning!\n",
    "\n",
    "We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4427b",
   "metadata": {},
   "source": [
    "# Let's make Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27462d6-0a1d-4771-b247-8d9f4adbd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "#custom_image_path = './stellar_images/test/galaxy/178(2).jpg \n",
    "custom_image_path = 'C:\\Users\\vishb\\Downloads\\Image-Classification-Using-Vision-Transformer\\Stellar_images\\Test\\galaxy-178.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e874df-5375-4a2b-b746-a1b9551b13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on custom image\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d0f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
